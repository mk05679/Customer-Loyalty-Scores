{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opendatasets\n",
      "  Downloading https://files.pythonhosted.org/packages/00/e7/12300c2f886b846375c78a4f32c0ae1cd20bdcf305b5ac45b8d7eceda3ec/opendatasets-0.1.22-py3-none-any.whl\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from opendatasets) (4.62.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from opendatasets) (7.0)\n",
      "Requirement already satisfied: kaggle in c:\\programdata\\anaconda3\\lib\\site-packages (from opendatasets) (1.5.12)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->opendatasets) (0.4.1)\n",
      "Requirement already satisfied: six>=1.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (1.12.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (2.8.0)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (2021.5.30)\n",
      "Requirement already satisfied: urllib3 in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (1.24.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (2.21.0)\n",
      "Requirement already satisfied: python-slugify in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (6.1.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kaggle->opendatasets) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kaggle->opendatasets) (2.8)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
      "Installing collected packages: opendatasets\n",
      "Successfully installed opendatasets-0.1.22\n"
     ]
    }
   ],
   "source": [
    "!pip install opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading elo-merchant-category-recommendation.zip to .\\elo-merchant-category-recommendation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615M/615M [02:27<00:00, 4.36MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting archive .\\elo-merchant-category-recommendation/elo-merchant-category-recommendation.zip to .\\elo-merchant-category-recommendation\n"
     ]
    }
   ],
   "source": [
    "od.download(\"https://www.kaggle.com/competitions/elo-merchant-category-recommendation/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zxp-rexqECKw",
    "outputId": "e5a52733-b8d8-4942-bfb0-26254d786a01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/fmfn/BayesianOptimization\n",
      "  Cloning https://github.com/fmfn/BayesianOptimization to c:\\users\\win\\appdata\\local\\temp\\pip-req-build-4p72ljj4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Error [WinError 2] The system cannot find the file specified while executing command git clone -q https://github.com/fmfn/BayesianOptimization C:\\Users\\Win\\AppData\\Local\\Temp\\pip-req-build-4p72ljj4\n",
      "Cannot find command 'git' - do you have 'git' installed and in your PATH?\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/fmfn/BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Using cached https://files.pythonhosted.org/packages/8f/51/d272fa065be3be615d279be915705fa3824b86155e36c974cfb8d3ceec1e/lightgbm-3.3.2.tar.gz\n",
      "Requirement already satisfied: wheel in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm) (0.33.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm) (1.16.2)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm) (1.2.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm) (0.21.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.0.1)\n",
      "Building wheels for collected packages: lightgbm\n",
      "  Building wheel for lightgbm (setup.py): started\n",
      "  Building wheel for lightgbm (setup.py): finished with status 'error'\n",
      "  Complete output from command C:\\ProgramData\\Anaconda3\\python.exe -u -c \"import setuptools, tokenize;__file__='C:\\\\Users\\\\Win\\\\AppData\\\\Local\\\\Temp\\\\pip-install-jc0miro0\\\\lightgbm\\\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d C:\\Users\\Win\\AppData\\Local\\Temp\\pip-wheel-on168uon --python-tag cp37:\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib\n",
      "  creating build\\lib\\lightgbm\n",
      "  copying lightgbm\\basic.py -> build\\lib\\lightgbm\n",
      "  copying lightgbm\\callback.py -> build\\lib\\lightgbm\n",
      "  copying lightgbm\\compat.py -> build\\lib\\lightgbm\n",
      "  copying lightgbm\\dask.py -> build\\lib\\lightgbm\n",
      "  copying lightgbm\\engine.py -> build\\lib\\lightgbm\n",
      "  copying lightgbm\\libpath.py -> build\\lib\\lightgbm\n",
      "  copying lightgbm\\plotting.py -> build\\lib\\lightgbm\n",
      "  copying lightgbm\\sklearn.py -> build\\lib\\lightgbm\n",
      "  copying lightgbm\\__init__.py -> build\\lib\\lightgbm\n",
      "  running egg_info\n",
      "  writing lightgbm.egg-info\\PKG-INFO\n",
      "  writing dependency_links to lightgbm.egg-info\\dependency_links.txt\n",
      "  writing requirements to lightgbm.egg-info\\requires.txt\n",
      "  writing top-level names to lightgbm.egg-info\\top_level.txt\n",
      "  reading manifest file 'lightgbm.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  no previously-included directories found matching 'build'\n",
      "  warning: no files found matching '*.so' under directory 'lightgbm'\n",
      "  warning: no files found matching '*.so' under directory 'compile'\n",
      "  warning: no files found matching '*.dll' under directory 'compile\\Release'\n",
      "  warning: no files found matching '*.dll' under directory 'compile\\windows\\x64\\DLL'\n",
      "  warning: no previously-included files matching '*.py[co]' found anywhere in distribution\n",
      "  warning: no previously-included files found matching 'compile\\external_libs\\compute\\.git'\n",
      "  writing manifest file 'lightgbm.egg-info\\SOURCES.txt'\n",
      "  copying lightgbm\\VERSION.txt -> build\\lib\\lightgbm\n",
      "  installing to build\\bdist.win32\\wheel\n",
      "  running install\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"C:\\Users\\Win\\AppData\\Local\\Temp\\pip-install-jc0miro0\\lightgbm\\setup.py\", line 378, in <module>\n",
      "      'Topic :: Scientific/Engineering :: Artificial Intelligence'])\n",
      "    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\setuptools\\__init__.py\", line 145, in setup\n",
      "      return distutils.core.setup(**attrs)\n",
      "    File \"C:\\ProgramData\\Anaconda3\\lib\\distutils\\core.py\", line 148, in setup\n",
      "      dist.run_commands()\n",
      "    File \"C:\\ProgramData\\Anaconda3\\lib\\distutils\\dist.py\", line 966, in run_commands\n",
      "      self.run_command(cmd)\n",
      "    File \"C:\\ProgramData\\Anaconda3\\lib\\distutils\\dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\wheel\\bdist_wheel.py\", line 228, in run\n",
      "      self.run_command('install')\n",
      "    File \"C:\\ProgramData\\Anaconda3\\lib\\distutils\\cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"C:\\ProgramData\\Anaconda3\\lib\\distutils\\dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"C:\\Users\\Win\\AppData\\Local\\Temp\\pip-install-jc0miro0\\lightgbm\\setup.py\", line 243, in run\n",
      "      raise Exception(\"Cannot install LightGBM in 32-bit Python, \"\n",
      "  Exception: Cannot install LightGBM in 32-bit Python, please use 64-bit Python instead.\n",
      "  \n",
      "  ----------------------------------------\n",
      "  Running setup.py clean for lightgbm\n",
      "Failed to build lightgbm\n",
      "Installing collected packages: lightgbm\n",
      "  Running setup.py install for lightgbm: started\n",
      "    Running setup.py install for lightgbm: finished with status 'error'\n",
      "    Complete output from command C:\\ProgramData\\Anaconda3\\python.exe -u -c \"import setuptools, tokenize;__file__='C:\\\\Users\\\\Win\\\\AppData\\\\Local\\\\Temp\\\\pip-install-jc0miro0\\\\lightgbm\\\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record C:\\Users\\Win\\AppData\\Local\\Temp\\pip-record-_96ovmhs\\install-record.txt --single-version-externally-managed --compile:\n",
      "    running install\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\Win\\AppData\\Local\\Temp\\pip-install-jc0miro0\\lightgbm\\setup.py\", line 378, in <module>\n",
      "        'Topic :: Scientific/Engineering :: Artificial Intelligence'])\n",
      "      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\setuptools\\__init__.py\", line 145, in setup\n",
      "        return distutils.core.setup(**attrs)\n",
      "      File \"C:\\ProgramData\\Anaconda3\\lib\\distutils\\core.py\", line 148, in setup\n",
      "        dist.run_commands()\n",
      "      File \"C:\\ProgramData\\Anaconda3\\lib\\distutils\\dist.py\", line 966, in run_commands\n",
      "        self.run_command(cmd)\n",
      "      File \"C:\\ProgramData\\Anaconda3\\lib\\distutils\\dist.py\", line 985, in run_command\n",
      "        cmd_obj.run()\n",
      "      File \"C:\\Users\\Win\\AppData\\Local\\Temp\\pip-install-jc0miro0\\lightgbm\\setup.py\", line 243, in run\n",
      "        raise Exception(\"Cannot install LightGBM in 32-bit Python, \"\n",
      "    Exception: Cannot install LightGBM in 32-bit Python, please use 64-bit Python instead.\n",
      "    \n",
      "    ----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Failed building wheel for lightgbm\n",
      "Command \"C:\\ProgramData\\Anaconda3\\python.exe -u -c \"import setuptools, tokenize;__file__='C:\\\\Users\\\\Win\\\\AppData\\\\Local\\\\Temp\\\\pip-install-jc0miro0\\\\lightgbm\\\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record C:\\Users\\Win\\AppData\\Local\\Temp\\pip-record-_96ovmhs\\install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in C:\\Users\\Win\\AppData\\Local\\Temp\\pip-install-jc0miro0\\lightgbm\\\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5hrQu3drDswD"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-5e2f3b0b28e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_squared_log_error\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;31m#Folds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRepeatedKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency,chisquare\n",
    "from pandas.plotting import scatter_matrix\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "from datetime import datetime, date\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "import pickle\n",
    "import prettytable\n",
    "from functools import reduce\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "# metrics\n",
    "from sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "#Folds\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedKFold\n",
    "from bayes_opt import BayesianOptimization\n",
    "#cross_validations\n",
    "from sklearn.model_selection import KFold #for K-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score #score evaluation\n",
    "from sklearn.model_selection import cross_val_predict #prediction\n",
    "from sklearn.model_selection import cross_validate\n",
    "#Modules for processors\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-aosfhIEMXQ"
   },
   "source": [
    "**2. Function to reduce the memory usageLoading Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ze2zCbFEDtt4"
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(data_frame,verbose=True):\n",
    "  numerals=['int16','int32','int64','float16','float32','float64']# Here we define the datatypes numerals.\n",
    "  initial_memory_usage=data_frame.memory_usage().sum()/ 1024**2# here we first calculate the memory usage by each feature and then sum up all the memory usage of all features in dataframe. Then we divide it by 2 power 12. So that memory usage will decrease by the dataframe.\n",
    "  for k in data_frame.columns:#for each column in each dataframe\n",
    "    datatype_feature= data_frame[k].dtypes# Here we found out the datatypes of each feature in dataframe.\n",
    "    if datatype_feature in numerals:# Here we check the feature lies in the category of numerics feature.\n",
    "      minimum_feature_value= data_frame[k].min()# if the feature is in numerics then first we get the  minimum of that feature.\n",
    "      maximum_feature_value= data_frame[k].max()# if the feature is in numerics then first we get the  maximum of that feature.\n",
    "      if str(datatype_feature)[:3]== 'int':#  Here we check that third feature is integer feature or not.\n",
    "        if minimum_feature_value > np.iinfo(np.int8).min and maximum_feature_value < np.iinfo(np.int8).max:# IF integer values is in the between of c_min and c_max and have the value of int8.means of (-128 to 127).\n",
    "          data_frame[k]= data_frame[k].astype(np.int8)# here simple we assign the int8 value in his corresponding integer value feature.\n",
    "        elif minimum_feature_value > np.iinfo(np.int16).min and maximum_feature_value < np.iinfo(np.int16).max:# Here simple we check for the int16 means(-32,768 to 32,767) bit range.\n",
    "          data_frame[k]= data_frame[k].astype(np.int16)# Here we assign the int16 values.  \n",
    "        elif minimum_feature_value > np.iinfo(np.int32).min and maximum_feature_value < np.iinfo(np.int32).max:# Here for int32 means(-2,147,483,648 to 2,147,483,647) bit range.\n",
    "          data_frame[k]= data_frame[k].astype(np.int32)#  Here we assign the int32 values.\n",
    "        elif minimum_feature_value > np.iinfo(np.int64).min and maximum_feature_value < np.iinfo(np.int64).max:# here we assign the int64 means(-9,223,372,036,854,775,808 to 9,223,372,036,854,775,807) bit range.\n",
    "          data_frame[k]= data_frame[k].astype(np.int64)# Here we assign the int64 values.\n",
    "      else:  \n",
    "        if minimum_feature_value > np.finfo(np.float16).min and maximum_feature_value < np.finfo(np.float16).max:# Otherwise if it is the float16 values range\n",
    "          data_frame[k]= data_frame[k].astype(np.float16)# assign it corresponding values.\n",
    "        elif minimum_feature_value > np.finfo(np.float32).min and maximum_feature_value < np.finfo(np.float32).max:# Here we check the min and max range of float 32 values.\n",
    "          data_frame[k]= data_frame[k].astype(np.float32)# assign the corresponding values.\n",
    "        else:\n",
    "          data_frame[k] = data_frame[k].astype(np.float64)# similarly for int64. \n",
    "  final_memory_usage= data_frame.memory_usage().sum()/1024**2# Here we again decrease the memory usage of each feature after separating the corresponding bit integer and float values\n",
    "  if verbose:\n",
    "    print(' The  decrease in memory usage from {:5.2f}MB to ({:.1f}% reduction)'.format(final_memory_usage,100* (initial_memory_usage-final_memory_usage)/initial_memory_usage))# Here we find out the difference between the memory decrease from the starting and end memory usage.\n",
    "  return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qu4kH8SyEXQn"
   },
   "source": [
    "**2.1 Function for getting the feature names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vrYhdCGWEU9h"
   },
   "outputs": [],
   "source": [
    "def get_new_columns(name,aggs):\n",
    "    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNBk7rtiEj6I"
   },
   "source": [
    "**2.2 Historical Transactions Aggregation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0xtsgW-Ebyx"
   },
   "outputs": [],
   "source": [
    "def historical_transaction():\n",
    "  '''\n",
    "  historical transactions aggregation function\n",
    "  '''\n",
    "  historical_transactions = pd.read_csv('historical_transactions.csv')# read historical transaction files\n",
    "  historical_transactions=reduce_mem_usage(historical_transactions)# reduce the memory usage of historical transaction\n",
    "  historical_transactions.replace([-np.inf,np.inf],np.nan,inplace=True)# replace the -inf,inf to nan values\n",
    "\n",
    "\n",
    "  historical_transactions['category_2'] = historical_transactions['category_2'].fillna(1,inplace=True)\n",
    "  historical_transactions['category_3'] = historical_transactions['category_3'].fillna('A',inplace=True)\n",
    "  historical_transactions['merchant_id'] = historical_transactions['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "  historical_transactions['installments'].replace(-1, np.nan,inplace=True)\n",
    "  historical_transactions['installments'].replace(999, np.nan,inplace=True)\n",
    "  historical_transactions['purchase_amount'] = historical_transactions['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
    "\n",
    "\n",
    "  # Transform A,B,C and Y, N into numerical form 1,2,3\n",
    "  historical_transactions['authorized_flag'] = historical_transactions['authorized_flag'].map({'Y': 1, 'N': 0})\n",
    "  historical_transactions['category_1'] = historical_transactions['category_1'].map({'Y': 1, 'N': 0})\n",
    "  historical_transactions['category_3'] = historical_transactions['category_3'].map({'A':0, 'B':1, 'C':2})\n",
    "  # Get the purchase date of the historical transaction\n",
    "  historical_transactions['purchase_date'] = pd.to_datetime(historical_transactions['purchase_date'])\n",
    "  #Here we get the week ordinal of the year\n",
    "  historical_transactions['weekofyear'] = historical_transactions['purchase_date'].dt.weekofyear\n",
    "  historical_transactions['month'] = historical_transactions['purchase_date'].dt.month# get the month\n",
    "  historical_transactions['day'] = historical_transactions['purchase_date'].dt.day# get the day\n",
    "  historical_transactions['weekday'] = historical_transactions.purchase_date.dt.weekday# get the week day\n",
    "  historical_transactions['weekend'] = (historical_transactions.purchase_date.dt.weekday >=5).astype(int)# weekend\n",
    "  historical_transactions['hour'] =historical_transactions['purchase_date'].dt.hour# hour from the purchase date \n",
    "  # month diff is subtraction of purchase date from the today date\n",
    "  historical_transactions['month_diff'] = ((datetime.today() - historical_transactions['purchase_date']).dt.days)//30\n",
    "  historical_transactions['month_diff'] += historical_transactions['month_lag']\n",
    "  # Here we get the duration when we multipluy the purchase amount and month_diff\n",
    "  historical_transactions['duration'] = historical_transactions['purchase_amount']*historical_transactions['month_diff']\n",
    "  #amount_month ratio is when we divide the purchase amount from month_diff\n",
    "  historical_transactions['amount_month_ratio'] = historical_transactions['purchase_amount']/historical_transactions['month_diff']\n",
    "  # price is when we divide the purchase amount from installments\n",
    "  historical_transactions['price'] = historical_transactions['purchase_amount'] / historical_transactions['installments']\n",
    "  gc.collect()\n",
    "\n",
    "\n",
    "  aggs = {}# here we aggregate all the values of sum,min,max,mean,var,skew for all the features\n",
    "  aggs['purchase_amount'] = ['sum','max','min','mean','var','skew']\n",
    "  aggs['installments'] = ['sum','max','mean','var','skew']\n",
    "  aggs['purchase_date'] = ['max','min']\n",
    "  aggs['month_lag'] = ['max','min','mean','var','skew']\n",
    "  aggs['month_diff']=['max','min','mean','var','skew']\n",
    "  aggs['weekend'] = ['sum', 'mean']\n",
    "  aggs['weekday'] = ['sum', 'mean']\n",
    "  aggs['authorized_flag']= ['sum', 'mean']\n",
    "  aggs['category_1']= ['sum','mean', 'max','min']\n",
    "  aggs['card_id'] = ['size','count']\n",
    "  aggs['month']= ['nunique', 'mean', 'min', 'max']\n",
    "  aggs['hour']= ['nunique', 'mean', 'min', 'max']\n",
    "  aggs['weekofyear']= ['nunique', 'mean', 'min', 'max']\n",
    "  aggs['day']= ['nunique', 'mean', 'min', 'max']\n",
    "  aggs['subsector_id']= ['nunique']\n",
    "  aggs['merchant_id']= ['nunique']\n",
    "  aggs['merchant_category_id'] = ['nunique']\n",
    "  aggs['price'] =['sum','mean','max','min','var']\n",
    "  aggs['duration'] = ['mean','min','max','var','skew']\n",
    "  aggs['amount_month_ratio']=['mean','min','max','var','skew'] \n",
    "  # here we try to aggregate all the mean,max,min,sum into category_2, category_3.\n",
    "  for col in ['category_2','category_3']:\n",
    "    historical_transactions[col+'_mean'] = historical_transactions.groupby(historical_transactions[col])['purchase_amount'].agg('mean')\n",
    "    historical_transactions[col+'_max'] = historical_transactions.groupby(historical_transactions[col])['purchase_amount'].agg('max')\n",
    "    historical_transactions[col+'_min'] = historical_transactions.groupby(historical_transactions[col])['purchase_amount'].agg('min')\n",
    "    historical_transactions[col+'_sum'] = historical_transactions.groupby(historical_transactions[col])['purchase_amount'].agg('sum')\n",
    "    aggs[col+'_mean'] = ['mean']\n",
    "  gc.collect()      \n",
    "  # here we groupby the features by card_id and have new features.\n",
    "  new_columns = get_new_columns('hist',aggs)\n",
    "  historical_transactions_group = historical_transactions.groupby(['card_id']).agg(aggs)\n",
    "  historical_transactions_group.columns = new_columns\n",
    "  historical_transactions_group.reset_index(drop=False,inplace=True)\n",
    "  gc.collect()\n",
    "  return historical_transactions_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvOFbAgUE2zO"
   },
   "source": [
    "**2.3 New Transactions Aggregation Fucntion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Lqddk8WFZP7"
   },
   "outputs": [],
   "source": [
    "def new_merchants_transactions():\n",
    "  '''\n",
    "  new merchants transactions aggregation functions\n",
    "  '''\n",
    "  new_transactions = pd.read_csv('new_merchant_transactions.csv')# read the new merchants transactions files\n",
    "  new_transactions=reduce_mem_usage(new_transactions)# reduce the memory usage of new merchants transactions files\n",
    "  new_transactions.replace([-np.inf,np.inf],np.nan,inplace=True)# replace -inf,inf with nan values in new merchants files.\n",
    "\n",
    "  new_transactions['category_2'] = new_transactions['category_2'].fillna(1,inplace=True)\n",
    "  new_transactions['category_3'] = new_transactions['category_3'].fillna('A',inplace=True)\n",
    "  new_transactions['merchant_id'] = new_transactions['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "  new_transactions['installments'].replace(-1, np.nan,inplace=True)\n",
    "  new_transactions['installments'].replace(999, np.nan,inplace=True)\n",
    "  new_transactions['purchase_amount'] = new_transactions['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
    "\n",
    "\n",
    "  # Transform A,B,C and Y, N into numerical form 1,2,3\n",
    "  new_transactions['authorized_flag'] = new_transactions['authorized_flag'].map({'Y': 1, 'N': 0})\n",
    "  new_transactions['category_1'] = new_transactions['category_1'].map({'Y': 1, 'N': 0})\n",
    "  new_transactions['category_3'] = new_transactions['category_3'].map({'A':0, 'B':1, 'C':2})\n",
    "  # Get the purchase date of the historical transaction\n",
    "  new_transactions['purchase_date'] = pd.to_datetime(new_transactions['purchase_date'])\n",
    "  #Here we get the week ordinal of the year\n",
    "  new_transactions['weekofyear'] = new_transactions['purchase_date'].dt.weekofyear\n",
    "  new_transactions['month'] = new_transactions['purchase_date'].dt.month# get the month\n",
    "  new_transactions['day'] = new_transactions['purchase_date'].dt.day# get the day\n",
    "  new_transactions['weekday'] = new_transactions.purchase_date.dt.weekday# get the week day\n",
    "  new_transactions['weekend'] = (new_transactions.purchase_date.dt.weekday >=5).astype(int)# weekend\n",
    "  new_transactions['hour'] =new_transactions['purchase_date'].dt.hour# hour from the purchase date \n",
    "  # month diff is subtraction of purchase date from the today date\n",
    "  new_transactions['month_diff'] = ((datetime.today() - new_transactions['purchase_date']).dt.days)//30\n",
    "  new_transactions['month_diff'] += new_transactions['month_lag']\n",
    "  # Here we get the duration when we multipluy the purchase amount and month_diff\n",
    "  new_transactions['duration'] = new_transactions['purchase_amount']*new_transactions['month_diff']\n",
    "  #amount_month ratio is when we divide the purchase amount from month_diff\n",
    "  new_transactions['amount_month_ratio'] = new_transactions['purchase_amount']/new_transactions['month_diff']\n",
    "  # price is when we divide the purchase amount from installments\n",
    "  new_transactions['price'] = new_transactions['purchase_amount'] / new_transactions['installments']\n",
    "  gc.collect()\n",
    "\n",
    "\n",
    "  aggs = {}# here we aggregate all the values of sum,min,max,mean,var,skew for all the features\n",
    "  aggs['purchase_amount'] = ['sum','max','min','mean','var','skew']\n",
    "  aggs['installments'] = ['sum','max','mean','var','skew']\n",
    "  aggs['purchase_date'] = ['max','min']\n",
    "  aggs['month_lag'] = ['max','min','mean','var','skew']\n",
    "  aggs['month_diff']=['max','min','mean','var','skew']\n",
    "  aggs['weekend'] = ['sum', 'mean']\n",
    "  aggs['weekday'] = ['sum', 'mean']\n",
    "  aggs['authorized_flag']= ['sum', 'mean']\n",
    "  aggs['category_1']= ['sum','mean', 'max','min']\n",
    "  aggs['card_id'] = ['size','count']\n",
    "  aggs['month']= ['nunique', 'mean', 'min', 'max']\n",
    "  aggs['hour']= ['nunique', 'mean', 'min', 'max']\n",
    "  aggs['weekofyear']= ['nunique', 'mean', 'min', 'max']\n",
    "  aggs['day']= ['nunique', 'mean', 'min', 'max']\n",
    "  aggs['subsector_id']= ['nunique']\n",
    "  aggs['merchant_id']= ['nunique']\n",
    "  aggs['merchant_category_id'] = ['nunique']\n",
    "  aggs['price'] =['sum','mean','max','min','var']\n",
    "  aggs['duration'] = ['mean','min','max','var','skew']\n",
    "  aggs['amount_month_ratio']=['mean','min','max','var','skew']\n",
    "  # here we try to aggregate all the mean,max,min,sum into category_2, category_3.\n",
    "  for col in ['category_2','category_3']:\n",
    "    new_transactions[col+'_mean'] = new_transactions.groupby(new_transactions[col])['purchase_amount'].agg('mean')\n",
    "    new_transactions[col+'_max'] = new_transactions.groupby(new_transactions[col])['purchase_amount'].agg('max')\n",
    "    new_transactions[col+'_min'] = new_transactions.groupby(new_transactions[col])['purchase_amount'].agg('min')\n",
    "    new_transactions[col+'_sum'] = new_transactions.groupby(new_transactions[col])['purchase_amount'].agg('sum')\n",
    "    aggs[col+'_mean'] = ['mean']\n",
    "  gc.collect()      \n",
    "  # here we groupby the features by card_id and have new features.\n",
    "  new_columns = get_new_columns('new_hist',aggs)\n",
    "  new_transactions_group = new_transactions.groupby(['card_id']).agg(aggs)\n",
    "  new_transactions_group.columns = new_columns\n",
    "  new_transactions_group.reset_index(drop=False,inplace=True)\n",
    "  gc.collect()\n",
    "  return new_transactions_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUAB2jv9FlMu"
   },
   "source": [
    "**2.4 Merchants Aggregate Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-mXDn_rFqWl"
   },
   "outputs": [],
   "source": [
    "def aggregation_merchants(merchants):\n",
    "  '''\n",
    "  merchants aggregation function\n",
    "  '''\n",
    "  second_mode=lambda x: Counter(x).most_common(1)[0][0]#Counting the most common element and take it as second mode.\n",
    "  # here we aggregate all the features of merchants with most common element and groupby according to merchant id.\n",
    "  group_merchants=merchants.groupby('merchant_id',as_index=False).agg({\n",
    "        \"merchant_group_id\": second_mode,\n",
    "        \"merchant_category_id\": second_mode,\n",
    "        \"subsector_id\": second_mode,\n",
    "        \"numerical_1\": \"mean\",\n",
    "        \"numerical_2\": \"mean\",\n",
    "        \"category_1\": second_mode,\n",
    "        \"most_recent_sales_range\": second_mode,\n",
    "        \"most_recent_purchases_range\": second_mode,\n",
    "        \"avg_sales_lag3\": \"mean\",\n",
    "        \"avg_purchases_lag3\": \"mean\",\n",
    "        \"active_months_lag3\": second_mode,\n",
    "        \"avg_sales_lag6\": \"mean\",\n",
    "        \"avg_purchases_lag6\": \"mean\",\n",
    "        \"active_months_lag6\": second_mode,\n",
    "        \"avg_sales_lag12\": \"mean\",\n",
    "        \"avg_purchases_lag12\": \"mean\",\n",
    "        \"active_months_lag12\": second_mode,\n",
    "        \"category_4\": second_mode,\n",
    "        \"city_id\": second_mode,\n",
    "        \"state_id\": second_mode,\n",
    "        \"category_2\": second_mode\n",
    "  })\n",
    "  return group_merchants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XnTsl1EFyuz"
   },
   "source": [
    "**2.5 New Merchant Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0-wWbI8F2ru"
   },
   "outputs": [],
   "source": [
    "# here we again aggregate the card_id because there are some dulplicate card_id.\n",
    "# here we aggregate the mean in numerical features of new merchants.\n",
    "def aggregation_transactions(transactions):\n",
    "  '''\n",
    "  new merchants mean encoded functions\n",
    "  '''\n",
    "  second_mode = lambda x: Counter(x).most_common(1)[0][0]#Counting the most common element and take it as second mode.\n",
    "  # here we aggregate all the features of merchants with most common element and groupby according to merchant id.\n",
    "  group_transactions = transactions.groupby(\"card_id\",as_index=False).agg({\n",
    "        \"authorized_flag\": second_mode,\n",
    "        \"merchant_category_id\": second_mode,\n",
    "        \"subsector_id\": second_mode,\n",
    "        \"merchant_id\":  second_mode,\n",
    "        \"category_1\": second_mode,\n",
    "        'month_lag': second_mode,\n",
    "        \"installments\": second_mode,\n",
    "        \"purchase_amount\": \"mean\",\n",
    "        \"city_id\": second_mode,\n",
    "        \"state_id\": second_mode,\n",
    "        \"category_2\": second_mode,\n",
    "        \"purchase_date\": second_mode\n",
    "  })\n",
    "  return group_transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRxnZoLuF4a9"
   },
   "source": [
    "**3. Mean encoding for train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6X7xrD-F6_e"
   },
   "outputs": [],
   "source": [
    "#https://medium.com/@narender.buchireddy/elo-merchant-category-recommendation-competition-on-kaggle-lightgbm-implementation-a-case-83b0912fa7f3\n",
    "def encoding_train(train, transactions, merchants, suffix):\n",
    "  '''\n",
    "  In this Function we encode the train data with mean of train data\n",
    "  '''\n",
    "  transactions=aggregation_transactions(transactions)# here we pass the new transactions data into the function to aggregate the most common values.\n",
    "  transactions.columns= [k+suffix if k not in ['card_id','merchant_id'] else k for k in transactions.columns]# here we try to get the colmns based on card_id and merchant_id.\n",
    "  # here we merge the train data with the most common new transactions feature values based 0on card_id.\n",
    "  transactions_train=pd.merge(train['card_id'],transactions,on='card_id',how='left')\n",
    "  del transactions# because we do not need that in future.\n",
    "\n",
    "  # merge the transaction and merchant data.\n",
    "  merchants = aggregation_merchants(merchants)# here we aggregate the merchant data.\n",
    "  # get the feature value on merchant_id\n",
    "  merchants.columns=[k+'_merchants'+suffix if k!='merchant_id' else k for k in merchants.columns]#\n",
    "  # here we merge the transaction_train with merchnats on the basis on merchant_id.\n",
    "  merge_merchants_transactions= pd.merge(transactions_train,merchants,on='merchant_id',how='left')\n",
    "  del merchants# delete the merchants because we have merge form of all the data.\n",
    "  del transactions_train# no need \n",
    "  del merge_merchants_transactions['merchant_id']# no need\n",
    "\n",
    "\n",
    "  train_card_id= train['card_id'].unique()# here we take the unique card_id of train data.\n",
    "  # check whether card_id is found is in all merge data of transactions and merchants.\n",
    "  INDEX= merge_merchants_transactions['card_id'].isin(list(train_card_id))\n",
    "  # filter the merge data for the unique card_id so that we are safe from duplicate card_id.\n",
    "  merge_merchants_transactions_unique=merge_merchants_transactions.loc[INDEX]\n",
    "  # Have the corresponding target value in card_id in merge merchant and transcations data.\n",
    "  merge_merchants_transactions_unique_label=merge_merchants_transactions_unique.merge(train[['card_id','target']],how='left',left_on='card_id',right_on='card_id')\n",
    "  del merge_merchants_transactions_unique\n",
    "  gc.collect()\n",
    "\n",
    "  # here we impute the missing value in merge merchants transactions because there are some missing values.\n",
    "  merge_merchants_transactions.fillna(-9999,inplace=True)\n",
    "\n",
    "  # now this is the time for mean encoding in train data\n",
    "  # here we add the suffix to the features to make new features so that we induct the values which we have from aggregation.\n",
    "  encoding_train_features=['authorized_flag'+suffix, 'category_1'+suffix, 'month_lag'+suffix, 'installments'+suffix,'category_2'+suffix,\n",
    "        'numerical_1_merchants'+suffix, 'category_1_merchants'+suffix, 'most_recent_sales_range_merchants'+suffix,\n",
    "        'most_recent_purchases_range_merchants'+suffix, 'active_months_lag3_merchants'+suffix, \n",
    "        'active_months_lag6_merchants'+suffix, 'active_months_lag12_merchants'+suffix, 'category_4_merchants'+suffix,\n",
    "        'category_2_merchants'+suffix]\n",
    "  for i in  encoding_train_features:\n",
    "    encoding= merge_merchants_transactions_unique_label.groupby(i).target.mean()\n",
    "    merge_merchants_transactions[i+'_mean_encoded']= merge_merchants_transactions[i].map(encoding)\n",
    "  del encoding\n",
    "  gc.collect()\n",
    "  return merge_merchants_transactions  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OR-7HIiKGAQY"
   },
   "source": [
    "**4. Train function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pO9nu--GGCld"
   },
   "outputs": [],
   "source": [
    "def train_func(train):\n",
    "  '''\n",
    "  get all the train features in this functions\n",
    "  '''\n",
    "  #train = pd.read_csv('train.csv')# Read the train file\n",
    "  train['outliers'] = 0\n",
    "  train.loc[train['target'] < -30, 'outliers'] = 1\n",
    "  train['outliers'].value_counts()\n",
    "  # Here we are doing mean encoding for features with outliers\n",
    "  for k in ['feature_1','feature_2','feature_3']:\n",
    "    label = train.groupby([k])['outliers'].mean()\n",
    "    train[k+\"_mean\"] = train[k].map(label) \n",
    "   # Now extract the days and Qtr\n",
    "  sr = pd.Series(train['first_active_month'])# Convert the first active month in series format\n",
    "  sr=pd.to_datetime(sr)\n",
    "  train['days'] = (date(2018, 2,1) - sr.dt.date).dt.days# just having error so we first try the numeric method\n",
    "  train['quarter'] = sr.dt.quarter# here we get the quarter.\n",
    "  k_cols = ['feature_1', 'feature_2', 'feature_3']\n",
    "  for i in k_cols:\n",
    "    train['days_' + i] = train['days'] * train[i]\n",
    "    train['days_' + i + '_ratio'] = train[i] / train['days']\n",
    "  \n",
    "  historical_trans=historical_transaction()\n",
    "\n",
    "  new_merchants_trans=new_merchants_transactions() \n",
    "  train = pd.merge(train,historical_trans,on='card_id',how='left')\n",
    "  train=train.drop(['hist_category_2_mean_mean','hist_category_3_mean_mean'],axis=1) \n",
    "  train = pd.merge(train,new_merchants_trans,on='card_id',how='left')\n",
    "  train=train.drop(['new_hist_category_2_mean_mean','new_hist_category_3_mean_mean'],axis=1)\n",
    "  gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "  #Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "  train['hist_purchase_date_max'] = pd.to_datetime(train['hist_purchase_date_max'])\n",
    "  train['hist_purchase_date_min'] = pd.to_datetime(train['hist_purchase_date_min'])\n",
    "  train['hist_purchase_date_diff'] = (train['hist_purchase_date_max'] - train['hist_purchase_date_min']).dt.days\n",
    "  train['hist_purchase_date_average'] = train['hist_purchase_date_diff']/train['hist_card_id_size']\n",
    "  train['hist_purchase_date_uptonow'] = (datetime.today() - train['hist_purchase_date_max']).dt.days\n",
    "  train['hist_purchase_date_uptomin'] = (datetime.today() - train['hist_purchase_date_min']).dt.days\n",
    "  sr = pd.Series(train['first_active_month'])\n",
    "  sr=pd.to_datetime(sr)\n",
    "  train['hist_first_buy'] = (train['hist_purchase_date_min'] - sr).dt.days\n",
    "  train['hist_last_buy'] = (train['hist_purchase_date_max'] - sr).dt.days\n",
    "\n",
    "  # here we convert this features in integer format for training in historical data.\n",
    "  for feature in ['hist_purchase_date_max','hist_purchase_date_min']:\n",
    "    train[feature] = train[feature].astype(np.int64) * 1e-9\n",
    "  \n",
    "  # Additional features \n",
    "  # https://www.kaggle.com/mfjwr1/simple-lightgbm-without-blending \n",
    "  train['new_hist_purchase_date_max'] = pd.to_datetime(train['new_hist_purchase_date_max'])\n",
    "  train['new_hist_purchase_date_min'] = pd.to_datetime(train['new_hist_purchase_date_min'])\n",
    "  train['new_hist_purchase_date_diff'] = (train['new_hist_purchase_date_max'] - train['new_hist_purchase_date_min']).dt.days\n",
    "  train['new_hist_purchase_date_average'] = train['new_hist_purchase_date_diff']/train['new_hist_card_id_size']\n",
    "  train['new_hist_purchase_date_uptonow'] = (datetime.today() - train['new_hist_purchase_date_max']).dt.days\n",
    "  train['new_hist_purchase_date_uptomin'] = (datetime.today() - train['new_hist_purchase_date_min']).dt.days\n",
    "  sr = pd.Series(train['first_active_month'])\n",
    "  sr=pd.to_datetime(sr)\n",
    "  train['new_first_buy'] = (train['new_hist_purchase_date_min'] - sr).dt.days\n",
    "  train['new_last_buy'] = (train['new_hist_purchase_date_max'] - sr).dt.days\n",
    "  train['new_hist_purchase_date_max'] = pd.to_numeric(train['new_hist_purchase_date_max'], errors='coerce')\n",
    "  train['new_hist_purchase_date_min'] = pd.to_numeric(train['new_hist_purchase_date_min'], errors='coerce')\n",
    "  train = train.dropna(subset=['new_hist_purchase_date_max'])\n",
    "  train = train.dropna(subset=['new_hist_purchase_date_min'])\n",
    "  # here we convert this features in integer format for training in new historical data.\n",
    "  for feature in ['new_hist_purchase_date_max','new_hist_purchase_date_min']:\n",
    "    train[feature] = train[feature].astype(np.int64) * 1e-9    \n",
    "  gc.collect()\n",
    "  \n",
    "  train['card_id_total'] = train['new_hist_card_id_size']+train['hist_card_id_size']\n",
    "  train['card_id_cnt_total'] = train['new_hist_card_id_count']+train['hist_card_id_count']\n",
    "  train['card_id_cnt_ratio'] = train['new_hist_card_id_count']/train['hist_card_id_count']\n",
    "  train['purchase_amount_total'] = train['new_hist_purchase_amount_sum']+train['hist_purchase_amount_sum']\n",
    "  train['purchase_amount_mean'] = train['new_hist_purchase_amount_mean']+train['hist_purchase_amount_mean']\n",
    "  train['purchase_amount_max'] = train['new_hist_purchase_amount_max']+train['hist_purchase_amount_max']\n",
    "  train['purchase_amount_min'] = train['new_hist_purchase_amount_min']+train['hist_purchase_amount_min']\n",
    "  train['purchase_amount_ratio'] = train['new_hist_purchase_amount_sum']/train['hist_purchase_amount_sum']\n",
    "  train['month_diff_mean'] = train['new_hist_month_diff_mean']+train['hist_month_diff_mean']\n",
    "  train['month_diff_ratio'] = train['new_hist_month_diff_mean']/train['hist_month_diff_mean']\n",
    "  train['month_lag_mean'] = train['new_hist_month_lag_mean']+train['hist_month_lag_mean']\n",
    "  train['month_lag_max'] = train['new_hist_month_lag_max']+train['hist_month_lag_max']\n",
    "  train['month_lag_min'] = train['new_hist_month_lag_min']+train['hist_month_lag_min']\n",
    "  train['category_1_mean'] = train['new_hist_category_1_mean']+train['hist_category_1_mean']\n",
    "  train['installments_total'] = train['new_hist_installments_sum']+train['hist_installments_sum']\n",
    "  train['installments_mean'] = train['new_hist_installments_mean']+train['hist_installments_mean']\n",
    "  train['installments_max'] = train['new_hist_installments_max']+train['hist_installments_max']\n",
    "  train['installments_ratio'] = train['new_hist_installments_sum']/train['hist_installments_sum']\n",
    "  train['price_total'] = train['purchase_amount_total'] / train['installments_total']\n",
    "  train['price_mean'] = train['purchase_amount_mean'] / train['installments_mean']\n",
    "  train['price_max'] =train['purchase_amount_max'] / train['installments_max']\n",
    "  train['duration_mean'] = train['new_hist_duration_mean']+train['hist_duration_mean']\n",
    "  train['duration_min'] = train['new_hist_duration_min']+train['hist_duration_min']\n",
    "  train['duration_max'] = train['new_hist_duration_max']+train['hist_duration_max']\n",
    "  train['amount_month_ratio_mean']=train['new_hist_amount_month_ratio_mean']+train['hist_amount_month_ratio_mean']\n",
    "  train['amount_month_ratio_min']=train['new_hist_amount_month_ratio_min']+train['hist_amount_month_ratio_min']\n",
    "  train['amount_month_ratio_max']=train['new_hist_amount_month_ratio_max']+train['hist_amount_month_ratio_max']\n",
    "  train['new_CLV'] = train['new_hist_card_id_count'] * train['new_hist_purchase_amount_sum'] / train['new_hist_month_diff_mean']\n",
    "  train['hist_CLV'] =train['hist_card_id_count'] * train['hist_purchase_amount_sum'] / train['hist_month_diff_mean']\n",
    "  train['CLV_ratio'] = train['new_CLV'] / train['hist_CLV']\n",
    "  gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "  print(' The data frame train shape is:',train.shape)\n",
    "  print(' Here we perform the mean encoding')\n",
    "\n",
    "  # load the merchant data.\n",
    "  merchants = reduce_mem_usage(pd.read_csv('/content/merchants.csv',nrows=1000))\n",
    "  merchants.replace([-np.inf, np.inf], np.nan, inplace=True)# here we replace the -inf,inf to nan value in merchnat data.\n",
    "  transactions = reduce_mem_usage(pd.read_csv('/content/new_merchant_transactions.csv',nrows=1000))\n",
    "  transactions.replace([-np.inf, np.inf], np.nan, inplace=True)\n",
    "\n",
    "  # Mean encoding on train data.\n",
    "  mean_encoding_train=encoding_train(train, transactions, merchants, suffix='_new')\n",
    "  train= pd.merge(train,mean_encoding_train,on='card_id',how='left')\n",
    "  del mean_encoding_train\n",
    "  print(' This is the shape of data after mean encoding is:',train.shape)\n",
    "\n",
    "\n",
    "  # mean encoding on merchants_historical_transactions.\n",
    "  merchants = reduce_mem_usage(pd.read_csv('/content/merchants.csv',nrows=1000))\n",
    "  merchants.replace([-np.inf, np.inf], np.nan, inplace=True)# here we replace the -inf,inf to nan value in merchnat data.\n",
    "  data_frame_historical_transactions=reduce_mem_usage(pd.read_csv('/content/historical_transactions.csv',nrows=1000))\n",
    "  data_frame_historical_transactions.replace([-np.inf, np.inf], np.nan, inplace=True)\n",
    "\n",
    "  # meaN encoding on historical_transactions\n",
    "  mean_encoding_hist=encoding_train(train, data_frame_historical_transactions, merchants, suffix='_hist')\n",
    "  print(' We also been done by the historical mean encoding')\n",
    "  train=pd.merge(train,mean_encoding_hist, on='card_id',how='left')\n",
    "  del mean_encoding_hist\n",
    "  train.head()\n",
    "\n",
    "  columns_of_train = ['authorized_flag_new', 'category_1_new', 'month_lag_new', 'installments_new','category_2_new',\n",
    "          'numerical_1_merchants_new', 'category_1_merchants_new', 'most_recent_sales_range_merchants_new',\n",
    "          'most_recent_purchases_range_merchants_new', 'active_months_lag3_merchants_new', \n",
    "          'active_months_lag6_merchants_new', 'active_months_lag12_merchants_new', 'category_4_merchants_new',\n",
    "          'category_2_merchants_new', \n",
    "          'authorized_flag_hist', 'category_1_hist', 'month_lag_hist', 'installments_hist', 'category_2_hist',\n",
    "          'numerical_1_merchants_hist', 'category_1_merchants_hist', 'most_recent_sales_range_merchants_hist',\n",
    "          'most_recent_purchases_range_merchants_hist', 'active_months_lag3_merchants_hist', \n",
    "          'active_months_lag6_merchants_hist', 'active_months_lag12_merchants_hist', 'category_4_merchants_hist',\n",
    "          'category_2_merchants_hist']\n",
    "\n",
    "\n",
    "  # one hot encoding for train data.\n",
    "  for i in train.columns:# for each column in train columns.\n",
    "    if train[i].isna().any():# here we check that which column have the nan values \n",
    "      train[i].replace([-np.inf, np.inf, np.nan], train[i].min(), inplace=True)# here we replace the nan values with min value found in column.\n",
    "  \n",
    "  #train = pd.concat([train, pd.get_dummies(train[columns_of_train], prefix = 'ohe_').astype(np.int8)], axis=1)\n",
    "  #Here we delete the basic columns.\n",
    "  columns_delete = ['feature_1','feature_2','feature_3', 'authorized_flag_new', 'merchant_category_id_new',\n",
    "       'subsector_id_new', 'category_1_new', 'month_lag_new',\n",
    "       'installments_new', 'purchase_amount_new', 'city_id_new',\n",
    "       'state_id_new', 'category_2_new', 'purchase_date_new',\n",
    "       'merchant_group_id_merchants_new', 'merchant_category_id_merchants_new',\n",
    "       'subsector_id_merchants_new', 'numerical_1_merchants_new',\n",
    "       'numerical_2_merchants_new', 'category_1_merchants_new',\n",
    "       'most_recent_sales_range_merchants_new',\n",
    "       'most_recent_purchases_range_merchants_new',\n",
    "       'avg_sales_lag3_merchants_new', 'avg_purchases_lag3_merchants_new',\n",
    "       'active_months_lag3_merchants_new', 'avg_sales_lag6_merchants_new',\n",
    "       'avg_purchases_lag6_merchants_new', 'active_months_lag6_merchants_new',\n",
    "       'avg_sales_lag12_merchants_new', 'avg_purchases_lag12_merchants_new',\n",
    "       'active_months_lag12_merchants_new', 'category_4_merchants_new',\n",
    "       'city_id_merchants_new', 'state_id_merchants_new',\n",
    "       'category_2_merchants_new', 'authorized_flag_hist',\n",
    "       'merchant_category_id_hist', 'subsector_id_hist', 'category_1_hist',\n",
    "       'month_lag_hist', 'installments_hist', 'purchase_amount_hist',\n",
    "       'city_id_hist', 'state_id_hist', 'category_2_hist',\n",
    "       'purchase_date_hist', 'merchant_group_id_merchants_hist',\n",
    "       'merchant_category_id_merchants_hist', 'subsector_id_merchants_hist',\n",
    "       'numerical_1_merchants_hist', 'numerical_2_merchants_hist',\n",
    "       'category_1_merchants_hist', 'most_recent_sales_range_merchants_hist',\n",
    "       'most_recent_purchases_range_merchants_hist',\n",
    "       'avg_sales_lag3_merchants_hist', 'avg_purchases_lag3_merchants_hist',\n",
    "       'active_months_lag3_merchants_hist', 'avg_sales_lag6_merchants_hist',\n",
    "       'avg_purchases_lag6_merchants_hist',\n",
    "       'active_months_lag6_merchants_hist', 'avg_sales_lag12_merchants_hist',\n",
    "       'avg_purchases_lag12_merchants_hist',\n",
    "       'active_months_lag12_merchants_hist', 'category_4_merchants_hist',\n",
    "       'city_id_merchants_hist', 'state_id_merchants_hist',\n",
    "       'category_2_merchants_hist'] \n",
    "\n",
    "  train = train.drop(columns_delete, axis = 1)# here we drop all these columns from train data.\n",
    "  train =  train.loc[:, train.columns[0:200]]# here we select first 200 features from the train data\n",
    "\n",
    "  omit = ['first_active_month', 'target','merchant_id', 'card_id', 'outliers','hist_purchase_date_max', 'hist_purchase_date_min', 'hist_card_id_size','new_purchase_date_max', 'new_purchase_date_min', 'new_card_id_size', 'target_f']\n",
    "  data_frame_columns = [i for i in train.columns if i not in omit]\n",
    "  train=train[data_frame_columns]\n",
    "  return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gpqJZaZGREc"
   },
   "source": [
    "**5. Final functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLqWYLRLGkFL"
   },
   "source": [
    "**5.1 First Predict Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qid5HoZJGQ0h"
   },
   "outputs": [],
   "source": [
    "def First_Final_Function(train):\n",
    "  data_frame_tr= train_func(train)# Here we get the  preprocessed data.\n",
    "  saved_model='lgbm_rep_kfold.sav'# lgbm model\n",
    "  lgbm_model= pd.read_pickle(saved_model)# read the model\n",
    "  predictions = lgbm_model.predict(data_frame_tr)# prediction on the data.\n",
    "  return predictions# here we return the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cIYVcF_Gr5Y"
   },
   "source": [
    "**5.2 Second Predict Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9isPDYtGvW6"
   },
   "outputs": [],
   "source": [
    "def Second_Final_Functions(train,target):\n",
    "  predict= First_Final_Function(train)\n",
    "  print('The Actual value for target variable is:',target)\n",
    "  print('The predicted target value is:',predict)\n",
    "  print(\"The root mean square error is: {:<8.5f}\".format(mean_squared_error(predict, target)**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H9TswD9THAnO",
    "outputId": "ac722317-107f-4302-aea5-392e0c572e14"
   },
   "outputs": [],
   "source": [
    "train=reduce_mem_usage(pd.read_csv('train.csv',parse_dates=[\"first_active_month\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeZC1VUoHGDR"
   },
   "source": [
    "**6 Test single data point on RMSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wnyQg8GwHawe",
    "outputId": "81f07562-941e-4025-a69e-b2399dca7642"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Raw_data=train[0:1]\n",
    "target=train['target'][3:4]\n",
    "Second_Final_Functions(Raw_data,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRvoeY5IHcm7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
